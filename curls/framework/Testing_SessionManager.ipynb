{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from pprint import pprint\n",
    "import numpy as np\n",
    "from SessionManager import SessionManager, Session, Episode, Agent\n",
    "from sqlalchemy import desc, asc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm = SessionManager()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_get_parent_from_rule: reward-max\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nathan/.conda/envs/tensorflow/lib/python3.6/site-packages/gym/envs/registration.py:14: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n",
      "  result = entry_point.load(False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_get_parent_from_rule: reward-max\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nathan/.conda/envs/tensorflow/lib/python3.6/site-packages/gym/envs/registration.py:14: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n",
      "  result = entry_point.load(False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_get_parent_from_rule: reward-max\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nathan/.conda/envs/tensorflow/lib/python3.6/site-packages/gym/envs/registration.py:14: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n",
      "  result = entry_point.load(False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_get_parent_from_rule: reward-max\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nathan/.conda/envs/tensorflow/lib/python3.6/site-packages/gym/envs/registration.py:14: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n",
      "  result = entry_point.load(False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_get_parent_from_rule: reward-max\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nathan/.conda/envs/tensorflow/lib/python3.6/site-packages/gym/envs/registration.py:14: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n",
      "  result = entry_point.load(False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_get_parent_from_rule: reward-max\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nathan/.conda/envs/tensorflow/lib/python3.6/site-packages/gym/envs/registration.py:14: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n",
      "  result = entry_point.load(False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_get_parent_from_rule: reward-max\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nathan/.conda/envs/tensorflow/lib/python3.6/site-packages/gym/envs/registration.py:14: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n",
      "  result = entry_point.load(False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_get_parent_from_rule: reward-max\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nathan/.conda/envs/tensorflow/lib/python3.6/site-packages/gym/envs/registration.py:14: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n",
      "  result = entry_point.load(False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_get_parent_from_rule: reward-max\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nathan/.conda/envs/tensorflow/lib/python3.6/site-packages/gym/envs/registration.py:14: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n",
      "  result = entry_point.load(False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_get_parent_from_rule: reward-max\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nathan/.conda/envs/tensorflow/lib/python3.6/site-packages/gym/envs/registration.py:14: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n",
      "  result = entry_point.load(False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_get_parent_from_rule: reward-max\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nathan/.conda/envs/tensorflow/lib/python3.6/site-packages/gym/envs/registration.py:14: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n",
      "  result = entry_point.load(False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_get_parent_from_rule: reward-max\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nathan/.conda/envs/tensorflow/lib/python3.6/site-packages/gym/envs/registration.py:14: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n",
      "  result = entry_point.load(False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_get_parent_from_rule: reward-max\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nathan/.conda/envs/tensorflow/lib/python3.6/site-packages/gym/envs/registration.py:14: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n",
      "  result = entry_point.load(False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_get_parent_from_rule: reward-max\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nathan/.conda/envs/tensorflow/lib/python3.6/site-packages/gym/envs/registration.py:14: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n",
      "  result = entry_point.load(False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_get_parent_from_rule: reward-max\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nathan/.conda/envs/tensorflow/lib/python3.6/site-packages/gym/envs/registration.py:14: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n",
      "  result = entry_point.load(False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_get_parent_from_rule: reward-max\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nathan/.conda/envs/tensorflow/lib/python3.6/site-packages/gym/envs/registration.py:14: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n",
      "  result = entry_point.load(False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_get_parent_from_rule: reward-max\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nathan/.conda/envs/tensorflow/lib/python3.6/site-packages/gym/envs/registration.py:14: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n",
      "  result = entry_point.load(False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_get_parent_from_rule: reward-max\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nathan/.conda/envs/tensorflow/lib/python3.6/site-packages/gym/envs/registration.py:14: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n",
      "  result = entry_point.load(False)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-5b6e2b1eaeb2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0msm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'--env-name'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'CartPole-v1'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'--episodes'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'25'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0msm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitialize_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0msm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Reinforcement-Learning-Framework/SessionManager.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    172\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'observations'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 174\u001b[0;31m                     \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    175\u001b[0m                 batch = {\n\u001b[1;32m    176\u001b[0m                     \u001b[0;34m\"observations\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Reinforcement-Learning-Framework/Agent.py\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self, observations, actions, discounted_rewards, *args, **kargs)\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m         \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m         \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.conda/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/training/optimizer.py\u001b[0m in \u001b[0;36mapply_gradients\u001b[0;34m(self, grads_and_vars, global_step, name)\u001b[0m\n\u001b[1;32m    591\u001b[0m                        ([str(v) for _, v, _ in converted_grads_and_vars],))\n\u001b[1;32m    592\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 593\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_slots\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvar_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    594\u001b[0m     \u001b[0mupdate_ops\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/training/adam.py\u001b[0m in \u001b[0;36m_create_slots\u001b[0;34m(self, var_list)\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0;31m# Create slots for the first and second moments.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvar_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 135\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_zeros_slot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"m\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    136\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_zeros_slot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"v\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/training/optimizer.py\u001b[0m in \u001b[0;36m_zeros_slot\u001b[0;34m(self, var, slot_name, op_name)\u001b[0m\n\u001b[1;32m   1137\u001b[0m     \u001b[0mnamed_slots\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slot_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mslot_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1138\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_var_key\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvar\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnamed_slots\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1139\u001b[0;31m       \u001b[0mnew_slot_variable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mslot_creator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_zeros_slot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1140\u001b[0m       self._restore_slot_variable(\n\u001b[1;32m   1141\u001b[0m           \u001b[0mslot_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mslot_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvariable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvar\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/training/slot_creator.py\u001b[0m in \u001b[0;36mcreate_zeros_slot\u001b[0;34m(primary, name, dtype, colocate_with_primary)\u001b[0m\n\u001b[1;32m    181\u001b[0m     return create_slot_with_initializer(\n\u001b[1;32m    182\u001b[0m         \u001b[0mprimary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitializer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mslot_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 183\u001b[0;31m         colocate_with_primary=colocate_with_primary)\n\u001b[0m\u001b[1;32m    184\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprimary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvariables\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mVariable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/training/slot_creator.py\u001b[0m in \u001b[0;36mcreate_slot_with_initializer\u001b[0;34m(primary, initializer, shape, dtype, name, colocate_with_primary)\u001b[0m\n\u001b[1;32m    158\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m       return _create_slot_var(primary, initializer, \"\", validate_shape, shape,\n\u001b[0;32m--> 160\u001b[0;31m                               dtype)\n\u001b[0m\u001b[1;32m    161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type_arg, value_arg, traceback_arg)\u001b[0m\n\u001b[1;32m   2316\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype_arg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue_arg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraceback_arg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2317\u001b[0m     self._cached_pure_variable_scope.__exit__(\n\u001b[0;32m-> 2318\u001b[0;31m         type_arg, value_arg, traceback_arg)\n\u001b[0m\u001b[1;32m   2319\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_current_name_scope\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2320\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_current_name_scope\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype_arg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue_arg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraceback_arg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type_arg, value_arg, traceback_arg)\u001b[0m\n\u001b[1;32m   1903\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_var_scope_store\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariable_scopes_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_old_subscopes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1904\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1905\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_var_scope_store\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose_variable_subscopes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_new_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1906\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_var_scope_store\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_scope\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_old\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1907\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36mclose_variable_subscopes\u001b[0;34m(self, scope_name)\u001b[0m\n\u001b[1;32m   1323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1324\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mclose_variable_subscopes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscope_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1325\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariable_scopes_count\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1326\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mscope_name\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscope_name\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"/\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1327\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariable_scopes_count\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i in range(100):\n",
    "    sm.configure(['--env-name', 'CartPole-v1', '--episodes', '25'])\n",
    "    sm.initialize_session()\n",
    "    sm.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'agent_id': 1,\n",
      " 'average_reward': 1.8,\n",
      " 'commit': 'fa33483a0b4bf62868a84ec29abc97b191eb9ada',\n",
      " 'created_at': '2019-02-03 21:48:41.472770',\n",
      " 'env_name': 'Test',\n",
      " 'episode_iterations': 100,\n",
      " 'git_url': 'git@github.com:nathanmargaglio/Reinforcement-Learning-Framework.git',\n",
      " 'id': 4,\n",
      " 'iteration': 1,\n",
      " 'parent_id': 1,\n",
      " 'save': False,\n",
      " 'updated_at': '2019-02-03 21:48:48.594734'}\n",
      "{'agent_id': 1,\n",
      " 'average_reward': None,\n",
      " 'commit': 'c2d06e02c0ed22104a17fe05d31410634182c26f',\n",
      " 'created_at': '2019-02-05 13:32:57.244276',\n",
      " 'env_name': 'Test',\n",
      " 'episode_iterations': 100,\n",
      " 'git_url': 'git@github.com:nathanmargaglio/Reinforcement-Learning-Framework.git',\n",
      " 'id': 6,\n",
      " 'iteration': 1,\n",
      " 'parent_id': 1,\n",
      " 'save': False,\n",
      " 'updated_at': 'None'}\n",
      "{'agent_id': 1,\n",
      " 'average_reward': 1.63,\n",
      " 'commit': 'fa33483a0b4bf62868a84ec29abc97b191eb9ada',\n",
      " 'created_at': '2019-02-03 21:48:34.012067',\n",
      " 'env_name': 'Test',\n",
      " 'episode_iterations': 100,\n",
      " 'git_url': 'git@github.com:nathanmargaglio/Reinforcement-Learning-Framework.git',\n",
      " 'id': 3,\n",
      " 'iteration': 1,\n",
      " 'parent_id': 1,\n",
      " 'save': False,\n",
      " 'updated_at': '2019-02-03 21:48:41.366665'}\n",
      "{'agent_id': 1,\n",
      " 'average_reward': 4.252,\n",
      " 'commit': 'c2d06e02c0ed22104a17fe05d31410634182c26f',\n",
      " 'created_at': '2019-02-05 14:52:28.233264',\n",
      " 'env_name': 'Test',\n",
      " 'episode_iterations': 1000,\n",
      " 'git_url': 'git@github.com:nathanmargaglio/Reinforcement-Learning-Framework.git',\n",
      " 'id': 13,\n",
      " 'iteration': 3,\n",
      " 'parent_id': 12,\n",
      " 'save': False,\n",
      " 'updated_at': '2019-02-05 14:54:15.376128'}\n",
      "{'agent_id': 1,\n",
      " 'average_reward': 1.835,\n",
      " 'commit': 'c2d06e02c0ed22104a17fe05d31410634182c26f',\n",
      " 'created_at': '2019-02-05 13:33:02.621248',\n",
      " 'env_name': 'Test',\n",
      " 'episode_iterations': 100,\n",
      " 'git_url': 'git@github.com:nathanmargaglio/Reinforcement-Learning-Framework.git',\n",
      " 'id': 7,\n",
      " 'iteration': 1,\n",
      " 'parent_id': 1,\n",
      " 'save': False,\n",
      " 'updated_at': '2019-02-05 13:41:25.818512'}\n",
      "{'agent_id': 1,\n",
      " 'average_reward': 4.214,\n",
      " 'commit': 'c2d06e02c0ed22104a17fe05d31410634182c26f',\n",
      " 'created_at': '2019-02-05 14:54:27.126844',\n",
      " 'env_name': 'Test',\n",
      " 'episode_iterations': 1000,\n",
      " 'git_url': 'git@github.com:nathanmargaglio/Reinforcement-Learning-Framework.git',\n",
      " 'id': 14,\n",
      " 'iteration': 4,\n",
      " 'parent_id': 13,\n",
      " 'save': False,\n",
      " 'updated_at': '2019-02-05 14:56:18.057443'}\n",
      "{'agent_id': 1,\n",
      " 'average_reward': 1.97,\n",
      " 'commit': 'fa33483a0b4bf62868a84ec29abc97b191eb9ada',\n",
      " 'created_at': '2019-02-03 21:48:26.141031',\n",
      " 'env_name': 'Test',\n",
      " 'episode_iterations': 100,\n",
      " 'git_url': 'git@github.com:nathanmargaglio/Reinforcement-Learning-Framework.git',\n",
      " 'id': 2,\n",
      " 'iteration': 1,\n",
      " 'parent_id': 1,\n",
      " 'save': False,\n",
      " 'updated_at': '2019-02-03 21:48:33.901848'}\n",
      "{'agent_id': 1,\n",
      " 'average_reward': 2.24,\n",
      " 'commit': 'c2d06e02c0ed22104a17fe05d31410634182c26f',\n",
      " 'created_at': '2019-02-05 13:42:07.167299',\n",
      " 'env_name': 'Test',\n",
      " 'episode_iterations': 100,\n",
      " 'git_url': 'git@github.com:nathanmargaglio/Reinforcement-Learning-Framework.git',\n",
      " 'id': 8,\n",
      " 'iteration': 1,\n",
      " 'parent_id': 1,\n",
      " 'save': False,\n",
      " 'updated_at': '2019-02-05 13:42:16.368777'}\n",
      "{'agent_id': 1,\n",
      " 'average_reward': 2.27,\n",
      " 'commit': 'fa33483a0b4bf62868a84ec29abc97b191eb9ada',\n",
      " 'created_at': '2019-02-03 21:48:17.921480',\n",
      " 'env_name': 'Test',\n",
      " 'episode_iterations': 100,\n",
      " 'git_url': 'git@github.com:nathanmargaglio/Reinforcement-Learning-Framework.git',\n",
      " 'id': 1,\n",
      " 'iteration': 0,\n",
      " 'parent_id': None,\n",
      " 'save': False,\n",
      " 'updated_at': '2019-02-03 21:48:25.997575'}\n",
      "{'agent_id': 1,\n",
      " 'average_reward': 3.923,\n",
      " 'commit': 'c2d06e02c0ed22104a17fe05d31410634182c26f',\n",
      " 'created_at': '2019-02-05 14:43:26.892085',\n",
      " 'env_name': 'Test',\n",
      " 'episode_iterations': 1000,\n",
      " 'git_url': 'git@github.com:nathanmargaglio/Reinforcement-Learning-Framework.git',\n",
      " 'id': 11,\n",
      " 'iteration': 1,\n",
      " 'parent_id': 1,\n",
      " 'save': False,\n",
      " 'updated_at': '2019-02-05 14:45:03.757593'}\n",
      "{'agent_id': 1,\n",
      " 'average_reward': 4.04,\n",
      " 'commit': 'c2d06e02c0ed22104a17fe05d31410634182c26f',\n",
      " 'created_at': '2019-02-05 14:47:59.735522',\n",
      " 'env_name': 'Test',\n",
      " 'episode_iterations': 1000,\n",
      " 'git_url': 'git@github.com:nathanmargaglio/Reinforcement-Learning-Framework.git',\n",
      " 'id': 12,\n",
      " 'iteration': 2,\n",
      " 'parent_id': 11,\n",
      " 'save': False,\n",
      " 'updated_at': '2019-02-05 14:49:44.666875'}\n",
      "{'agent_id': 1,\n",
      " 'average_reward': 2.12,\n",
      " 'commit': 'c2d06e02c0ed22104a17fe05d31410634182c26f',\n",
      " 'created_at': '2019-02-05 14:42:30.030564',\n",
      " 'env_name': 'Test',\n",
      " 'episode_iterations': 100,\n",
      " 'git_url': 'git@github.com:nathanmargaglio/Reinforcement-Learning-Framework.git',\n",
      " 'id': 9,\n",
      " 'iteration': 1,\n",
      " 'parent_id': 1,\n",
      " 'save': False,\n",
      " 'updated_at': '2019-02-05 14:42:38.991630'}\n",
      "{'agent_id': 1,\n",
      " 'average_reward': None,\n",
      " 'commit': 'c2d06e02c0ed22104a17fe05d31410634182c26f',\n",
      " 'created_at': '2019-02-05 14:43:24.283012',\n",
      " 'env_name': 'Test',\n",
      " 'episode_iterations': 1000,\n",
      " 'git_url': 'git@github.com:nathanmargaglio/Reinforcement-Learning-Framework.git',\n",
      " 'id': 10,\n",
      " 'iteration': 1,\n",
      " 'parent_id': 1,\n",
      " 'save': False,\n",
      " 'updated_at': 'None'}\n",
      "{'agent_id': 1,\n",
      " 'average_reward': 2.04,\n",
      " 'commit': 'fa33483a0b4bf62868a84ec29abc97b191eb9ada',\n",
      " 'created_at': '2019-02-03 21:49:31.473192',\n",
      " 'env_name': 'Test',\n",
      " 'episode_iterations': 100,\n",
      " 'git_url': 'git@github.com:nathanmargaglio/Reinforcement-Learning-Framework.git',\n",
      " 'id': 5,\n",
      " 'iteration': 1,\n",
      " 'parent_id': 1,\n",
      " 'save': False,\n",
      " 'updated_at': '2019-02-03 21:49:40.512359'}\n",
      "{'agent_id': 1,\n",
      " 'average_reward': 4.326,\n",
      " 'commit': 'c2d06e02c0ed22104a17fe05d31410634182c26f',\n",
      " 'created_at': '2019-02-05 14:59:22.420849',\n",
      " 'env_name': 'Test',\n",
      " 'episode_iterations': 1000,\n",
      " 'git_url': 'git@github.com:nathanmargaglio/Reinforcement-Learning-Framework.git',\n",
      " 'id': 15,\n",
      " 'iteration': 4,\n",
      " 'parent_id': 13,\n",
      " 'save': False,\n",
      " 'updated_at': '2019-02-05 15:01:17.866983'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sessions = sm.db.query(Session).all()\n",
    "[pprint(s()) for s in sessions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss = sm.db.query(Session).filter(Session.average_reward != None).order_by(desc(Session.average_reward)).limit(1).first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 4,\n",
       " 'agent_id': 1,\n",
       " 'iteration': 1,\n",
       " 'parent_id': 1,\n",
       " 'env_name': 'CartPole-v1',\n",
       " 'commit': 'feef789b3aead6f5694a3038254d1f2bf2bfc377',\n",
       " 'gir_url': 'git@github.com:nathanmargaglio/Reinforcement-Learning-Framework.git',\n",
       " 'average_reward': 90.26,\n",
       " 'episode_iterations': 100,\n",
       " 'created_at': '2019-02-03 16:24:40.579593',\n",
       " 'updated_at': '2019-02-03 16:25:55.800923'}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm.db.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[61.07, 83.13, 112.35]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[s.average_reward for s in sessions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 1,\n",
       " 'agent_id': 1,\n",
       " 'iteration': 0,\n",
       " 'parent_id': None,\n",
       " 'env_name': 'CartPole-v1',\n",
       " 'commit': 'feef789b3aead6f5694a3038254d1f2bf2bfc377',\n",
       " 'gir_url': 'git@github.com:nathanmargaglio/Reinforcement-Learning-Framework.git',\n",
       " 'average_reward': 112.35,\n",
       " 'episode_iterations': 100,\n",
       " 'created_at': '2019-02-03 16:15:36.126489',\n",
       " 'updated_at': '2019-02-03 16:17:58.330766'}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "session = sessions[-1]\n",
    "session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'config': {},\n",
      " 'created_at': '2019-02-03 21:48:17.852505',\n",
      " 'id': 1,\n",
      " 'name': None,\n",
      " 'save': False,\n",
      " 'summary': None}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agents = sm.db.query(Agent).all()\n",
    "[pprint(a()) for a in agents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
